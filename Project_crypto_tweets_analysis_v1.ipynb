{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo - will remove this section before submittion \n",
    "* change code using without for loops\n",
    "\n",
    "Kavya github link\n",
    "https://github.com/KavyaOS/Cryptocurrency_Analysis_PricePrediction.git\n",
    "\n",
    "Errors while running Kavya nottebook:\n",
    "1.  ModuleNotFoundError: No module named 'plotly'\n",
    "sol: \n",
    "pip install plotly\n",
    "\n",
    "2. ModuleNotFoundError: No module named 'keras'\n",
    "\n",
    "sol:\n",
    "pip install keras\n",
    "\n",
    "3. ImportError: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`\n",
    "pip install tensorflow\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prof notes - \n",
    "Given that everyone going different things for the project I want you to add a short  description in your notebook. There are up to six things that I want.\n",
    "\n",
    "First if you are working on a team then include the names and Red IDs of both team members at the top of the readme file. Just have one person submit the project so I don’t end up grading the same project twice.\n",
    "\n",
    "Second a brief description of what the project does. What are the goals of your assignment? Did you succeed? What did you learn. In the assignments I knew what you were to do. So if you just had code and tables I could figure things out. In your project you need to explain what your code is doing and what the output of your code means. In Jupyter notebooks you can use markdown cells to contain text. Use the markdown cells to to explain your code and results as they are easier to read then comments in code. Use the markdown cells to add headers (Introduction, How to Run, Results, etc) to give the notebook some structure.  \n",
    "\n",
    "Third any special instructions that I need to run the project. \n",
    "\n",
    "Fourth if you use any third party libraries list them and give the pip command needed to install them. A sentence or two on what the library does for you. If your idea comes from a site like Kaggle include a link to it. \n",
    "\n",
    "Fifth a list know issues with the project. If a feature is not working let me know. If a feature is not working I will grade it more severe than if you do not tell me.\n",
    "\n",
    "Six do not upload large data files. If your total upload to the course portal is 5 MB you will be fine. If people start uploading large files to the course portal the server will crash causing you and other people in the class to panic. (I need to rewrite that server.) If you data file puts you over that limit add a command in your jupyter notebook that will  download your data file from its source. The following command in a code cell on a Unix machine will download covid data at the given https address to the local file covid.csv.\n",
    "\n",
    "\n",
    "!curl -o covid.csv https://usafactsstatic.blob.core.windows.net/public/data/covid-19/covid_confirmed_usafacts.csv \n",
    "\n",
    "\n",
    "If your code takes a long time to run on the original data set then I would like a smaller dataset to run while grading. If the file is small enough it can be combined with your notebook and up loaded. If not provide a method to download the file: dropbox link, Google drive link, etc. If you are not able to do this contact me to make other arrangements. Always include a link to access the original data set file.\n",
    "\n",
    "\n",
    "As with assignments and the exam you will submit the project to the course portal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Big Data CS649 Project -  Kavya and Sachin Kumar (RedIds - 825893660 and 823431551)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Info\n",
    "\n",
    "### Objective :\n",
    "- Analysing the trend of crypto currency for the last few years.\n",
    "- Cryptocurrency price Prediction using Deep learning. (for example ML models - Linear Regression,  K-Nearest Neighbors.)\n",
    "- Analysis of social media on the crypto prize. Sentiments using few keywords in the tweets and relate with the currency price.  Positive, negative or neutral emotions.\n",
    "- Our main focus would be on Twitter and Bit-coin. We can explore more currencies and platforms if time permits. \n",
    "\n",
    "\n",
    "### Datasets :\n",
    "- Tweets dataset : https://www.kaggle.com/alaix14/bitcoin-tweets-20160101-to-20190329\n",
    "    - Contains all the bitcoin tweets\n",
    "    - date range is from 2016-Jan-01 to 2019-March-29\n",
    "    \n",
    "### Special Library/Packages installed : \n",
    "- TextBlob\n",
    "    - pip install -U textblob\n",
    "    - https://textblob.readthedocs.io/en/dev/install.html\n",
    "- Language detector and translator\n",
    "    - pip install google_trans_new\n",
    "    - https://github.com/lushan88a/google_trans_new\n",
    "- pip install pandas-profiling\n",
    "- conda install graphviz\n",
    "- #todo : might need to delete below packages\n",
    "- pip install atoti \n",
    "- pip install atoti-aws\n",
    "\n",
    "\n",
    "\n",
    "### Approach :\n",
    "\n",
    "\n",
    "### Chalenges :\n",
    " - Not able to install few packages on windows like whatthelang\n",
    "    - Language Prediction\n",
    "    - https://github.com/indix/whatthelang\n",
    "    - pip install -r requirements.txt\n",
    "    - pip install whatthelang\n",
    " - Google translator stopped translating the tweets after certain time with below error.\n",
    "    - Error - \"Our systems have detected unusual traffic from your computer network\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages that needs to be installed before running this notebook. Uncomment below lines once to install it and then comment it again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U textblob\n",
    "# ! pip install google_trans_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sk105659\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download english stopwords, this might take little time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few configurations before running the notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Dataset input file path ###########################################################################\n",
    "bitcoin_tweets_dataset_filepath = '../input_datasets/tweets.csv'   # Input dataset which needt to downloaded from Kaggle as given above.\n",
    "  \n",
    "\n",
    "################## Number of records to be processed from the dataset input file ###########################\n",
    "# this is important as the dataset is very huge and we have many bottleneck services like - \n",
    "# 1) google lan detector\n",
    "# 2) google lan translatoin, \n",
    "# 3) removing english verbose words from tweet text using stopwords from nltk.corpus\n",
    "# 4) finding sentiments out of english words using TextBlob\n",
    "\n",
    "no_of_records = 99999   # This seems to be a decent number as per our anlysis - output & performance\n",
    "# no_of_records = 1000  \n",
    "\n",
    "\n",
    "######### Goggle translation flag to enable english translation or not#################################\n",
    "#   todo : detecotor reached to limit after certain hits\n",
    "# if get this error : google_new_transError: 429 (Too Many Requests) from TTS API. then please change below flag from 0 to 1. \n",
    "# Doing so, it will not convert non-english to english lan and therefore the sentiments calucations are not 100% accurate.\n",
    "\n",
    "# Its a common issue for all free user : https://github.com/lushan88a/google_trans_new/issues/28\n",
    "# we have tried many options like giving differnt proxies and making connection using private VPN and keep switching the VPN tunnels (specially to Mexico worked) to get the desired ouput for 100000 records.\n",
    "\n",
    "is_eng_translator_limit_reached = 0   # 0 if limit not recached, 1 if limit reached, \n",
    "is_detect_lang_required = 0           # 0 if not required , 1 if required. Turning this off is good as sometime it slows or reach to thresold limit faster than translation itself\n",
    "\n",
    "\n",
    "# Also note, once the translation is done, the output would be saved on the disk. \n",
    "# and so for all the future runs, no translation is required, ranslated data would be read from the saved disk.\n",
    "\n",
    "processed_bitcoin_tweets_dataset_filepath = './datasets/tweets_processed_v50s_eng.csv'  # Name of the file after google translation work is done once. \n",
    "# processed_bitcoin_tweets_dataset_filepath = '../input_datasets/tweets_processed_test10_eng.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "\n",
    "def read_and_clean_input_file(input_file):\n",
    "    \n",
    "    # todo : using small dataset for the devlopment purpose\n",
    "    bitcoin_tweets_dataset_df_raw = pd.read_csv(\n",
    "        input_file, \n",
    "        sep=';',\n",
    "        nrows=no_of_records,\n",
    "#         dtype={'text': str},\n",
    "        converters={'text': str},\n",
    "        usecols=['timestamp', 'text'],                            \n",
    "        index_col=[\"timestamp\"])\n",
    "    \n",
    "#     display(bitcoin_tweets_dataset_df_raw)\n",
    "#     display(bitcoin_tweets_dataset_df_raw.info())\n",
    "\n",
    "    \n",
    "    # Strip time information from the column 'timestamp' if any\n",
    "#     bitcoin_tweets_dataset_df_raw['timestamp'] = pd.to_datetime(bitcoin_tweets_dataset_df_raw['timestamp']).dt.date\n",
    "    bitcoin_tweets_dataset_df_raw.index = pd.Series(pd.to_datetime(bitcoin_tweets_dataset_df_raw.index)).dt.date\n",
    "    \n",
    "#     display(bitcoin_tweets_dataset_df_raw)\n",
    "#     display(bitcoin_tweets_dataset_df_raw.info())\n",
    "    \n",
    "    # df['timestamp'] = df['timestamp'].astype('datetime64[ns]')\n",
    "    return bitcoin_tweets_dataset_df_raw\n",
    "\n",
    "\n",
    "def filter_by_daterange(df):\n",
    "    start_date = pd.to_datetime(\"2017-03-01\").date()\n",
    "    end_date = pd.to_datetime(\"2019-03-29\").date()\n",
    "    filtered_date_range = (df.index >= start_date) & (df.index <= end_date)\n",
    "    df = df[filtered_date_range]\n",
    "    ### Filter only Business day\n",
    "    is_business_day = BDay().is_on_offset\n",
    "    filtered_business_days = pd.to_datetime(df.index).map(is_business_day)\n",
    "    df[filtered_business_days]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Below function is just for anlysing the dataset #####################################################\n",
    "    \n",
    "def analysing_dataset(df):\n",
    "    display(df)\n",
    "    display(df.info())\n",
    "    display(df.index.min())\n",
    "    display(df.index.max())\n",
    "#     finding maximum and minimum date\n",
    "    \n",
    "\n",
    "########### Testing block #####################################################################################################\n",
    "# bitcoin_tweets_dataset_df_cleaned = read_and_clean_input_file(bitcoin_tweets_dataset_filepath)\n",
    "# analysing_dataset(bitcoin_tweets_dataset_df_cleaned)\n",
    "# bitcoin_tweets_dataset_df_cleaned = filter_by_daterange(bitcoin_tweets_dataset_df_cleaned)\n",
    "# analysing_dataset(bitcoin_tweets_dataset_df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_trans_new import google_translator  \n",
    "\n",
    "# translator = google_translator()\n",
    "translator = google_translator(timeout=30)\n",
    "# translator = google_translator(url_suffix=\"https\",timeout=5,proxies={'https':'206.189.44.99:8080','http':'96.9.77.203:55667','https':'140.227.68.230:58888','https':'72.196.184.54:8081' })\n",
    "detector = google_translator()  \n",
    "\n",
    "def add_english_translated_tweets_to_df(df):\n",
    "    english_translated_col = []\n",
    "    df['text'] = df['text'].astype(str)\n",
    "    for tweet in df['text']:\n",
    "        if len(tweet)!=0:\n",
    "            if is_detect_lang_required == 1 :\n",
    "                print(\"detecting lan first\")\n",
    "                detect_lang = detector.detect(tweet)\n",
    "                if (detect_lang != ['en', 'english']) :\n",
    "                    english_translated_col.append(translatoror.translate(tweet,lang_tgt='en') )\n",
    "                else:\n",
    "                    english_translated_col.append(tweet)\n",
    "            else:\n",
    "                print(\"detecting lan not required\")\n",
    "                english_translated_col.append(translator.translate(tweet,lang_tgt='en') )\n",
    "        else:\n",
    "            english_translated_col.append(None)\n",
    "\n",
    "    df['english_translated'] = english_translated_col\n",
    "    return df\n",
    "\n",
    "############  Testing block  ##########################################\n",
    "\n",
    "# function to detect lan\n",
    "# detector = google_translator()  \n",
    "# text = \"È appena uscito un nuovo video! LES CRYPTOMONN...\"\n",
    "# text = \"Another text which is already in english, so should not be translated.\"\n",
    "# detect_lang = 'translate all text'\n",
    "# detect_lang = detector.detect(text)\n",
    "# print(detect_lang)\n",
    "\n",
    "\n",
    "# function to translate the text to english\n",
    "# if (detect_lang != ['en', 'english']) :\n",
    "#     print(translator.translate(text,lang_tgt='en') )\n",
    "# print(translator.translate(text,lang_tgt='en') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_clean_filter_translate_write_file(input_file):\n",
    "    bitcoin_tweets_dataset_df_cleaned = read_and_clean_input_file(input_file)\n",
    "    bitcoin_tweets_dataset_df_cleaned = filter_by_daterange(bitcoin_tweets_dataset_df_cleaned)\n",
    "    \n",
    "    if(is_eng_translator_limit_reached != 1):\n",
    "        bitcoin_tweets_dataset_df_cleaned = add_english_translated_tweets_to_df(bitcoin_tweets_dataset_df_cleaned)\n",
    "    else:        \n",
    "        bitcoin_tweets_dataset_df_cleaned['english_translated'] = bitcoin_tweets_dataset_df_cleaned['text']\n",
    "\n",
    "    ##### Write the processed ouput to disk. This is to avoid unnecssary english translation everytime we run the program\n",
    "    bitcoin_tweets_dataset_df_cleaned.to_csv(index=True, sep=',' , path_or_buf=processed_bitcoin_tweets_dataset_filepath)\n",
    "#     display(bitcoin_tweets_dataset_df_cleaned)\n",
    "    \n",
    "    \n",
    "############  Testing block  ##########################################  \n",
    "# read_clean_filter_translate_write_file(bitcoin_tweets_dataset_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed bitcoin_tweets dataset filep exist\n"
     ]
    }
   ],
   "source": [
    "# todo: Note the seperator delimeter. orginal file has ';' and processed file has ','. \n",
    "# Should be good across complete file\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "processed_bitcoin_tweets_dataset_filepath \n",
    "\n",
    "if path.exists(processed_bitcoin_tweets_dataset_filepath):\n",
    "    print (\"processed bitcoin_tweets dataset filep exist\")\n",
    "else:\n",
    "    print (\"processed bitcoin_tweets dataset file does not exist\")\n",
    "    read_clean_filter_translate_write_file(bitcoin_tweets_dataset_filepath)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>english_translated</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-03-12</th>\n",
       "      <td>#Bitcoin #Satoshi #crypto #blockchain #Airdrop...</td>\n",
       "      <td>#Bitcoin #Satoshi #crypto #blockchain #Airdrop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-11</th>\n",
       "      <td>I didnt tether at $7300 #iamspartacus\\n$btc $e...</td>\n",
       "      <td>I didnt tether at $7300 #iamspartacus\\n$btc $e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-21</th>\n",
       "      <td>#Bitcoin #Satoshi #crypto #blockchain #Airdrop...</td>\n",
       "      <td>#Bitcoin #Satoshi #crypto #blockchain #Airdrop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-09</th>\n",
       "      <td>#Bitcoin #Satoshi #crypto #blockchain #Airdrop...</td>\n",
       "      <td>#Bitcoin #Satoshi #crypto #blockchain #Airdrop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-12</th>\n",
       "      <td>#Bitcoin #crypto #Airdrop\\nNew Airdrop #p2pb2b...</td>\n",
       "      <td>#Bitcoin #crypto #Airdrop\\nNew Airdrop #p2pb2b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-28</th>\n",
       "      <td>How the \"Times\" have changed - great to see su...</td>\n",
       "      <td>How the \"Times\" have changed - great to see su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-08</th>\n",
       "      <td>My weekly #Bitcoin and #crypto report for Bitc...</td>\n",
       "      <td>My weekly #Bitcoin and #crypto report for Bitc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-25</th>\n",
       "      <td>Why gift cards are scams:\\n1) turns liquid cas...</td>\n",
       "      <td>Why gift cards are scams:\\n1) turns liquid cas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-08</th>\n",
       "      <td>Only 1% of the world's water supply is usable,...</td>\n",
       "      <td>Only 1% of the world's water supply is usable,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-03</th>\n",
       "      <td>Max Gravitt joins DRIFE as a Lead Blockchain D...</td>\n",
       "      <td>Max Gravitt joins DRIFE as a Lead Blockchain D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2014 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         text  \\\n",
       "timestamp                                                       \n",
       "2019-03-12  #Bitcoin #Satoshi #crypto #blockchain #Airdrop...   \n",
       "2018-09-11  I didnt tether at $7300 #iamspartacus\\n$btc $e...   \n",
       "2019-03-21  #Bitcoin #Satoshi #crypto #blockchain #Airdrop...   \n",
       "2019-03-09  #Bitcoin #Satoshi #crypto #blockchain #Airdrop...   \n",
       "2019-03-12  #Bitcoin #crypto #Airdrop\\nNew Airdrop #p2pb2b...   \n",
       "...                                                       ...   \n",
       "2018-12-28  How the \"Times\" have changed - great to see su...   \n",
       "2019-03-08  My weekly #Bitcoin and #crypto report for Bitc...   \n",
       "2018-12-25  Why gift cards are scams:\\n1) turns liquid cas...   \n",
       "2019-03-08  Only 1% of the world's water supply is usable,...   \n",
       "2019-03-03  Max Gravitt joins DRIFE as a Lead Blockchain D...   \n",
       "\n",
       "                                           english_translated  \n",
       "timestamp                                                      \n",
       "2019-03-12  #Bitcoin #Satoshi #crypto #blockchain #Airdrop...  \n",
       "2018-09-11  I didnt tether at $7300 #iamspartacus\\n$btc $e...  \n",
       "2019-03-21  #Bitcoin #Satoshi #crypto #blockchain #Airdrop...  \n",
       "2019-03-09  #Bitcoin #Satoshi #crypto #blockchain #Airdrop...  \n",
       "2019-03-12  #Bitcoin #crypto #Airdrop\\nNew Airdrop #p2pb2b...  \n",
       "...                                                       ...  \n",
       "2018-12-28  How the \"Times\" have changed - great to see su...  \n",
       "2019-03-08  My weekly #Bitcoin and #crypto report for Bitc...  \n",
       "2018-12-25  Why gift cards are scams:\\n1) turns liquid cas...  \n",
       "2019-03-08  Only 1% of the world's water supply is usable,...  \n",
       "2019-03-03  Max Gravitt joins DRIFE as a Lead Blockchain D...  \n",
       "\n",
       "[2014 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>english_translated</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-05-01</th>\n",
       "      <td>02/05/2018 - 00:00\\n=========================...</td>\n",
       "      <td>02/05/2018 - 00:00  ==========================...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-20</th>\n",
       "      <td>#monday #HorseRacing #tips\\nTip 1 Wolves 6:25 ...</td>\n",
       "      <td>#monday #HorseRacing #tips\\nTip 1 Wolves 6:25 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-07-07</th>\n",
       "      <td>またイベントで歌うことになりました笑\\nあのね、やっぱりね、エンタメで有名になりたいのよw\\...</td>\n",
       "      <td>I will sing at the event again  Well, after al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-03</th>\n",
       "      <td>New post in Verified Crypto News: #MarketCap\\n...</td>\n",
       "      <td>New post in Verified Crypto News: #MarketCap\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-27</th>\n",
       "      <td>Top 5 #cryptocurrencies \\n Alert Time: 2018-12...</td>\n",
       "      <td>Top 5 #cryptocurrencies \\n Alert Time: 2018-12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-05-08</th>\n",
       "      <td>Launch!!! Not later than October 2018, 20:00 (...</td>\n",
       "      <td>Launch!!! Not later than October 2018, 20:00 (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-07</th>\n",
       "      <td>こんばんは！2018-09-07 20:00 レート情報\\nBTC：707,236 (↓-1...</td>\n",
       "      <td>Good evening! 2018-09-07 20:00 Rate Informatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-25</th>\n",
       "      <td>#Bitcoin #thatBitcoinlife will provide everyth...</td>\n",
       "      <td>#Bitcoin #thatBitcoinlife will provide everyth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-23</th>\n",
       "      <td>#LMA $LMA Trading competition https://www.coin...</td>\n",
       "      <td>#LMA $LMA Trading competition https://www.coin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-09-12</th>\n",
       "      <td>Zapraszam na DARMOWY webinar pt: Jak czytać wy...</td>\n",
       "      <td>Welcome to FREE WEBINAR PT: How do I read char...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         text  \\\n",
       "timestamp                                                       \n",
       "2018-05-01   02/05/2018 - 00:00\\n=========================...   \n",
       "2019-01-20  #monday #HorseRacing #tips\\nTip 1 Wolves 6:25 ...   \n",
       "2018-07-07  またイベントで歌うことになりました笑\\nあのね、やっぱりね、エンタメで有名になりたいのよw\\...   \n",
       "2018-09-03  New post in Verified Crypto News: #MarketCap\\n...   \n",
       "2018-12-27  Top 5 #cryptocurrencies \\n Alert Time: 2018-12...   \n",
       "2018-05-08  Launch!!! Not later than October 2018, 20:00 (...   \n",
       "2018-09-07  こんばんは！2018-09-07 20:00 レート情報\\nBTC：707,236 (↓-1...   \n",
       "2017-05-25  #Bitcoin #thatBitcoinlife will provide everyth...   \n",
       "2018-09-23  #LMA $LMA Trading competition https://www.coin...   \n",
       "2018-09-12  Zapraszam na DARMOWY webinar pt: Jak czytać wy...   \n",
       "\n",
       "                                           english_translated  \n",
       "timestamp                                                      \n",
       "2018-05-01  02/05/2018 - 00:00  ==========================...  \n",
       "2019-01-20  #monday #HorseRacing #tips\\nTip 1 Wolves 6:25 ...  \n",
       "2018-07-07  I will sing at the event again  Well, after al...  \n",
       "2018-09-03  New post in Verified Crypto News: #MarketCap\\n...  \n",
       "2018-12-27  Top 5 #cryptocurrencies \\n Alert Time: 2018-12...  \n",
       "2018-05-08  Launch!!! Not later than October 2018, 20:00 (...  \n",
       "2018-09-07  Good evening! 2018-09-07 20:00 Rate Informatio...  \n",
       "2017-05-25  #Bitcoin #thatBitcoinlife will provide everyth...  \n",
       "2018-09-23  #LMA $LMA Trading competition https://www.coin...  \n",
       "2018-09-12  Welcome to FREE WEBINAR PT: How do I read char...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min date is  2017-05-24\n",
      "max date is  2019-03-29\n"
     ]
    }
   ],
   "source": [
    "processed_bitcoin_tweets_dataset_df = pd.read_csv(\n",
    "        processed_bitcoin_tweets_dataset_filepath, \n",
    "        sep=',',\n",
    "        dtype= {'text': str, 'english_translated': str},\n",
    "        # dtype= str,\n",
    "        # converters={'english_translated': str},\n",
    "        index_col=[\"timestamp\"])\n",
    "    \n",
    "# Strip time information from the column 'timestamp' if any\n",
    "# processed_bitcoin_tweets_dataset_df['timestamp'] = pd.to_datetime(processed_bitcoin_tweets_dataset_df['timestamp']).dt.date\n",
    "processed_bitcoin_tweets_dataset_df.index = pd.Series(pd.to_datetime(processed_bitcoin_tweets_dataset_df.index)).dt.date\n",
    "display(processed_bitcoin_tweets_dataset_df)\n",
    "\n",
    "\n",
    "# Filter the dates between a range which is common to both dataframes - tweets dataframe and price dataframe\n",
    "processed_bitcoin_tweets_dataset_df = filter_by_daterange(processed_bitcoin_tweets_dataset_df)\n",
    "# processed_bitcoin_tweets_dataset_df = processed_bitcoin_tweets_dataset_df.sort_index()   -- no need to sort. that is just for verification and display purpose, if needed.\n",
    "\n",
    "display(processed_bitcoin_tweets_dataset_df.sample(10))\n",
    "print('min date is ',processed_bitcoin_tweets_dataset_df.index.min())\n",
    "print('max date is ',processed_bitcoin_tweets_dataset_df.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sk105659\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\"\"\" Utility function to clean tweet text by removing special characters and links using regex\"\"\"\n",
    "def clean_special_chars(text):\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+://\\S+)\", \" \", text).split())\n",
    "\n",
    "\n",
    "\"\"\" Utility function to clean tweet text and remove verbose english words using nltk stopwords\"\"\"\n",
    "def clean_and_remove_english_verbose_words(text):\n",
    "    text = clean_special_chars(text)\n",
    "    forbidden_words = set(stopwords.words('english'))\n",
    "    text = ' '.join(text.split('.'))\n",
    "    text = re.sub('\\/',' ',text)\n",
    "    text = text.strip('\\'\"')\n",
    "    text = re.sub(r'@([^\\s]+)',r'\\1',text)\n",
    "    text = re.sub(r'\\\\',' ',text)\n",
    "    text = text.lower()\n",
    "    text = re.sub('[\\s]+', ' ', text)\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',text)\n",
    "    text = re.sub(r'((http)\\S+)','',text)\n",
    "    text = re.sub(r'\\s+', ' ', re.sub('[^A-Za-z]', ' ', text.strip().lower())).strip()\n",
    "    text = re.sub(r'\\W+', ' ', text.strip().lower()).strip()\n",
    "    text = [word for word in text.split() if word not in forbidden_words]\n",
    "    return ' '.join(text)\n",
    "\n",
    "\n",
    "\"\"\" Utility function to classify sentiment of passed tweet using textblob's sentiment method \"\"\"\n",
    "def sentiment(text):\n",
    "    # create TextBlob object of passed tweet text\n",
    "    text_analysis = TextBlob(text)\n",
    "    return text_analysis.sentiment.polarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "\"\"\" Utility function to classify sentiment of passed tweet using textblob's sentiment method \"\"\"\n",
    "def add_eng_keywords_and_sentiments(df):\n",
    "    english_keywords_col = []\n",
    "    sentiment_col = []\n",
    "    for tweet in df['english_translated']:\n",
    "        \n",
    "        # If running stopwords on big data slows donw the speed, then we can skip stopwords logic for performance and only use the simple cleaning of special chars\n",
    "        english_keywords = clean_and_remove_english_verbose_words(tweet)\n",
    "        #   english_keywords = clean_special_chars(tweet)\n",
    "        english_keywords_col.append(english_keywords)\n",
    "        \n",
    "        # Add corresponding sentiments\n",
    "        sentiment_polarity = sentiment(tweet)\n",
    "        sentiment_col.append(sentiment_polarity)\n",
    "    \n",
    "    df['english_keywords'] = english_keywords_col\n",
    "    df['sentiment'] = sentiment_col\n",
    "    return df\n",
    "\n",
    "\n",
    "# processed_bitcoin_tweets_dataset_df['english_keywords'] = processed_bitcoin_tweets_dataset_df['english_translated'].apply(lambda text: clean_and_remove_english_verbose_words(text))\n",
    "# processed_bitcoin_tweets_dataset_df['sentiment'] = processed_bitcoin_tweets_dataset_df['english_keywords'].apply(lambda text: sentiment(text)) # new column of sentiment\n",
    "# processed_bitcoin_tweets_dataset_df.drop(columns='english_translated', inplace=True)\n",
    "\n",
    "processed_bitcoin_tweets_dataset_df = add_eng_keywords_and_sentiments(processed_bitcoin_tweets_dataset_df)\n",
    "processed_bitcoin_tweets_dataset_df.sample(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter the dates between a range which is common to both dataframes - tweets dataframe and price dataframe\n",
    "# processed_bitcoin_tweets_dataset_df = filter_by_daterange(processed_bitcoin_tweets_dataset_df)\n",
    "# # processed_bitcoin_tweets_dataset_df = processed_bitcoin_tweets_dataset_df.sort_index()   -- no need to sort. that is just for verification and display purpose, if needed.\n",
    "# processed_bitcoin_tweets_dataset_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# considering the average tweet sentiment of the day.\n",
    "# df = df.groupby(df.index).agg(lambda x: x.value_counts().index[0])\n",
    "#  todo: played with first, max, min, mean. i think mean is the best one for this.\n",
    "bitcoin_tweets_sentiment_df = processed_bitcoin_tweets_dataset_df.groupby(processed_bitcoin_tweets_dataset_df.index).min()\n",
    "bitcoin_tweets_sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### from other notebook #######################\n",
    "%store -r big_frame\n",
    "big_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_price_df = big_frame.query(\"coin_name == 'Bitcoin'\")\n",
    "# todo : remove warning.. ... \n",
    "df1 = bitcoin_price_df['Date']\n",
    "x2  = bitcoin_price_df.loc[:,['Date']]\n",
    "x2\n",
    "bitcoin_price_df['Date'] = pd.to_datetime(df1).dt.date\n",
    "# bitcoin_price_df['Date'] = pd.to_datetime(x2).dt.date\n",
    "# bitcoin_price_df['Date'] = pd.to_datetime(x2.stack())\n",
    "\n",
    "bitcoin_price_df = bitcoin_price_df.set_index(['Date'])\n",
    "display(bitcoin_price_df)\n",
    "# display(bitcoin_price_df.info())\n",
    "\n",
    "# Filter the dates between a range. This is common to both dataframes - tweets dataframe and price dataframe\n",
    "bitcoin_price_df = filter_by_daterange(bitcoin_price_df)\n",
    "# bitcoin_price_df = bitcoin_price_df['Price'].to_frame()\n",
    "bitcoin_price_df = bitcoin_price_df[['Price', 'Change']]\n",
    "display(bitcoin_price_df)\n",
    "display(type(bitcoin_price_df))\n",
    "# display(bitcoin_price_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bitcoin_price_and_tweets_data(bitcoin_price_df, bitcoin_tweets_sentiment_df):\n",
    "    #Combine two dataframes based on time.\n",
    "    df = pd.merge(bitcoin_price_df, bitcoin_tweets_sentiment_df, left_index=True, right_index=True, how='inner')\n",
    "    return df\n",
    "\n",
    "def make_sentiment_column_categorical(bitcoin_price_sentiment):\n",
    "    # Adding 'Sentiment categorical' column.\n",
    "    bitcoin_price_sentiment['sentiment_cat'] = bitcoin_price_sentiment['sentiment'].astype('category')\n",
    "    bitcoin_price_sentiment['sentiment_cat'] = bitcoin_price_sentiment['sentiment_cat'].cat.codes\n",
    "    return bitcoin_price_sentiment\n",
    "\n",
    "\n",
    "bitcoin_price_sentiment  = merge_bitcoin_price_and_tweets_data(bitcoin_price_df, bitcoin_tweets_sentiment_df)\n",
    "bitcoin_price_sentiment['sentiment_scaled'] = bitcoin_price_sentiment['sentiment'] * 10000\n",
    "bitcoin_price_sentiment = make_sentiment_column_categorical(bitcoin_price_sentiment)\n",
    "bitcoin_price_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_two_lines(line1, line2, label1=None, label2=None, title='', xlabel=None, ylabel=None, legend_loc='best', lw=2):\n",
    "    fig, ax = plt.subplots(1, figsize=(13, 7))\n",
    "    ax.plot(line1, label=label1, linewidth=lw)\n",
    "    ax.plot(line2, label=label2, linewidth=lw)\n",
    "    ax.set_xlabel(xlabel, fontsize=14)\n",
    "    ax.set_ylabel(ylabel, fontsize=14)\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.legend(loc=legend_loc, fontsize=16)\n",
    "\n",
    "def plot_one_line(line1, label1=None, title='', xlabel=None, ylabel=None, legend_loc='best', lw=2):\n",
    "    fig, ax = plt.subplots(1, figsize=(13, 7))\n",
    "    ax.plot(line1, label=label1, linewidth=lw)\n",
    "    ax.set_xlabel(xlabel, fontsize=14)\n",
    "    ax.set_ylabel(ylabel, fontsize=14)\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.legend(loc=legend_loc, fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_one_line(bitcoin_price_sentiment['Price'], 'Price', title='Analysing bitcoin price ')\n",
    "plot_one_line(bitcoin_price_sentiment['sentiment'] , 'sentiment', title='Analysing tweets sentiments')\n",
    "plot_one_line(bitcoin_price_sentiment['sentiment_scaled'] , 'sentiment_scaled', title='Analysing tweets sentiments')\n",
    "plot_one_line(bitcoin_price_sentiment['sentiment_cat'] , 'sentiment_cat', title='Analysing tweets sentiments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_two_lines(bitcoin_price_sentiment['Price'], bitcoin_price_sentiment['sentiment'], 'Price', 'sentiment', title='Analysing tweets sentiments impact on bitcoin price ')\n",
    "plot_two_lines(bitcoin_price_sentiment['Price'], bitcoin_price_sentiment['sentiment_scaled'], 'Price', 'sentiment_scaled', title='Analysing tweets sentiments impact on bitcoin price ')\n",
    "plot_two_lines(bitcoin_price_sentiment['Price'], bitcoin_price_sentiment['sentiment_cat'], 'Price', 'sentiment_cat', title='Analysing tweets sentiments impact on bitcoin price ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_price_sentiment['sentiment_cat_scaled'] = bitcoin_price_sentiment['sentiment_cat'] * 130\n",
    "plot_two_lines(bitcoin_price_sentiment['Price'], bitcoin_price_sentiment['sentiment_cat_scaled'], 'Price', 'sentiment_cat_scaled', title='Analysing tweets sentiments impact on bitcoin price ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_price_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Cleaned tweets count = \", bitcoin_tweets_sentiment_df.sentiment.count())\n",
    "print(\"\\n Cleaned bitcoin_price count = \", bitcoin_price_df.Price.count())\n",
    "print(\"\\n Cleaned merged df count = \", bitcoin_price_sentiment.Price.count())\n",
    "\n",
    "def get_tweet_sentiment(tweet):\n",
    "    tweet_polarity = analyse_tweet(tweet)\n",
    "    # set sentiment\n",
    "    if tweet_polarity > 0:\n",
    "        return 'positive'\n",
    "    elif tweet_polarity == 0:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'negative'\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
